\section{Kompleksitet} \label{kap:kompleksitet}
%side 250 ish
%table 1 side 247 i pdf.

Der findes to former for kompleksitet, \emph{tidskompleksitet} og pladskompleksitet, men vi vælger at fokusere på tidskompleksitet. 
Tidskompleksitet fortæller noget om, hvordan udviklingen i tid af en given algoritme forløber, når størrelsen på datasættet stiger. I tidskompleksitet vil man ofte kigge på tre forskellige former for tilfælde. Disse tilfælde er \emph{bedste tilfælde}, \emph{værste tilfælde} og det \emph{gennemsnitlige tilfælde}. 
Bedste tilfælde beskriver algoritmen under optimale forhold, hvor datasættet er sat i den rækkefølge, der gør, at algoritmen tager mindst mulig tid. I en lineær søgealgoritme vil dette være, at elementet, der søges efter, er det første element i listen. 
Værste tilfælde er en situation, hvori datasættet er ordnet i den værste rækkefølge, så der skal gennemføres det største mulige antal operationer for den givne algoritme. Hvis vi igen tager eksemplet med den lineære søgealgoritme, ville det være, at elementet der søges efter, ikke findes i listen over elementer.
Det gennemsnitlige tilfælde ser på et gennemsnitligt datasæt, dette er dog meget svært at bestemme, da det er svært at bestemme nogle parametre, der gør, at man får et gennemsnitligt sæt.
Det mest sigende er det gennemsnitlige tilfælde, men da det er svært at bruge, vil man ofte nøjes med det værste tilfælde, da man så i alle tilfælde kan sige, at algoritmen har denne kompleksitet eller mindre.



\input{fig/tab/algo/tab_kompleksitet}

\autoref{tab:kompleksitet} viser, hvilken terminologi man vil bruge, når man snakker om de forskellige kompleksiteter. Får man i sin analyse kompleksiteten $\Theta(n)$, vil den kaldes lineær, da tiden, det vil tage for algoritmen at løse problemet, stiger lineært med inputstørrelsen.
For at beskrive disse algoritmer vil man bruge operatorerne \emph{store-O}, \emph{store-$\Omega$} og \emph{store-$\Theta$}. 
\subsection{Store-$O$}
Store-$O$ bliver brugt til at begrænse funktionen opadtil. 

\begin{defn}
$f(n)$ siges at være $O(g(n))$, hvis og kun hvis der eksisterer positive konstanter $C$ og $n_o$, så $|f(n)| \leq C |g(n)|$ for alle $n \geq n_o$.
\end{defn}

Man kan med garanti sige, at tiden, algoritmen tager, vil vokse lig $g(n)$ eller mindre, når inputstørrelsen stiger, hvis $f(n)=O(g(n))$.

\begin{exmp} \label{ex:store-O}
Vi tager i dette eksempel udgangspunkt i funktionen $f(n)=13n+3$. 
Her skal vi forsøge at finde vores $C$ og $g(n)$, så uligheden $|f(n)| \leq C|g(n)|$ gælder for alle $n$ over $n_0$. Hvis vi sætter $g(n) = n$ og $C=20$, står der $13n+3 \leq 20n$. Dette er ikke sandt ved $n=0$, da der vil stå $3 \leq 0$. For $n=1$ står der $13\cdot 1 + 3 \leq 20 \cdot 1$, hvilket er sandt, og dermed er vores ulighed sand for $n \geq 1$.
Når vi har fundet vores $C$, $g(n)$ og $n_0$, kan vi nu konkludere, at $f(n) = O(g(n))$, hvilket i vores eksempel er $f(n) = O(n)$.
Vi kan dermed sige, at kompleksiteten af $f(n)$ er lineær eller bedre.
\end{exmp}
Det vil også være korrekt at sige, at $g(n)$ er lig $n!$ eller en anden vilkårlig, hurtigere voksende funktion, men da man altid vil vælge den mest begrænsende funktion, vil $n$ være det mest præcise. 

\subsection{Store-$\Omega$}
\begin{defn}
$f(n)$ siges at være  $\Omega(g(n))$, hvis og kun hvis der eksisterer positive konstanter $C$ og $n_o$, så $|f(n)| \geq C |g(n)|$ for alle $n \geq n_o$.
\end{defn}
Store-$\Omega$ bruges, modsat store-$O$, til at begrænse funktionen nedadtil, altså finde den nedre grænse for algoritmen.
\begin{exmp}
Hvis vi igen i dette eksempel tager udgangspunkt i funktionen fra Eksempel \ref{ex:store-O}, $f(n)=13n+3$, skal vi nu forsøge at finde $C$ og $g(n)$, hvor det gælder, at det er lavere end $f(n)$ for alle $n \geq n_0$. 
Vi kan med fordel bruge den samme $g(n)$ som ovenfor, $g(n)=n$. Så skal vi finde en $C$-værdi, der gør uligheden sand. Hvis $C=1$, står der $13n+3 \geq 1n$. Dette må gøre sig gældende for alle $n \geq 0$. Derved har vi vores konstanter $C=1$ og $n_0 = 0$, og funktionen $g(n)=n$, og vi kan nu begrænse funktionen nedadtil med $g(n)$, altså $f(n)=\Omega(n)$.
\end{exmp}

På samme måde som ved store-$O$-notationen vil man her kunne sætte $g(n)$ lig med en vilkårlig, langsommere voksende funktion fx $\log n$, $n \log n$ med flere, men da man vil begrænse den så meget som muligt, vælger man den største funktion, hvor uligheden stadig er sand.
\subsection{Store-$\Theta$}
\begin{defn} \label{defn:store-Theta}
$f(n)$ siges at være $\Theta(g(x))$, hvis og kun hvis der eksisterer positive konstanter $C_1, C_2$ og $n_o$, så $C_1|g(n)| \leq |f(n)| \leq C_2|g(n)|$ for alle $n \geq n_o$.
\end{defn}
Hvis $f(n)$ både er $O(g(n))$ og $\Omega(g(n))$, kan man jfr. Definition \ref{defn:store-Theta} sige at $f(n) = \Theta(g(n))$. 
Der skal altså eksistere et $g(n)$, hvor man ved forskellige $C$-værdier kan begrænse $f(n)$ både opad- og nedadtil. 
\begin{exmp}
Hvis man tager udgangspunkt i de foregående eksempler for store-$O$ og store-$\Omega$, ser vi, at $g(n) = n$ i begge tilfælde med to forskellige $C$ værdier. \\
$f(n)=13n+3, \ g(n)=n, \ C_1=1, \ C_2=20$. \\
Disse værdier kan indsættes i uligheden, så vi får $1n \leq 13n+3 \leq 20n| \forall n \geq 1$. \\
$f(n)= \Theta(n)$.
\end{exmp}

I følgende afsnit tager vi udgangspunkt i to sorteringsalgoritmer og ser på store-$O$ i bedste og værste tilfælde.

\subsection{Kompleksitet af sorteringer} \label{kap:kompleksitet_sortering}

For at finde kompleksiteten af bubblesortering bruger vi store-$O$-notationen fra tidligere. 
Bubblesortering, som nævnt i \autoref{kap:sortering}, sammenligner to elementer fra listen og flytter rundt på dem, hvis de står i forkert rækkefølge.
Vi starter med værste tilfælde. Her ser vi på en liste, $P$, der er sorteret i omvendt rækkefølge, $(5,4,3,2,1)$.
I første iteration sammenligner den 5 med 4 og bytter om, 5 med 3, bytter igen, 5 med 2, bytter, og til sidst 5 med 1. Nu står 5 korrekt, og $(4, 3, 2, 1, 5)$.
Efter næste iteration er $(3, 2, 1, 4, 5)$, og de sidste iteration medfører hhv. $(2, 1, 3, 4, 5)$ og $(1, 2, 3, 4, 5)$. Her ses altså, at der itereres 4 gange, eller $n-1$ gange. 
For hver iteration, $i$, sammenligner den $n-i$ gange. 
Som vi kender fra tidligere teori, tælles de mindre ordner ikke med, derved er kompleksiteten $O(n\times n)$ eller $O(n^2)$.

I bedste tilfælde er listen $(1, 2, 3, 4, 5)$.
I vores algoritme, stopper algoritmen først, når den har lavet $n-1$ iterationer, og stopper dermed ikke på trods af at listen kan være i korrekt rækkefølge tidligere. I bedste tilfælde, vil den derfor være lige så kompleks som vores værste tilfælde, altså $O(n^2)$. Det vil dog være relativt nemt at optimere algoritmen, så den stopper, hvis den ikke laver flere ombytninger i en given iteration. Denne optimering vil gøre algoritmen $O(n)$ i bedste fald, da den i dette tilfælde kun gennemfører én iteration, og der stadig udføres $n-1$ sammenligninger i denne iteration. 
\\

En anden sorteringsalgoritme er indskudssorteringen fra \autoref{kap:sortering}. Denne tager udgangspunkt i et enkelt element ad gangen og sammenligner dette med resten af den korrekte liste.
Hvis vi igen benytter det værste tilfælde, når listen er $(1,2,3,4,5)$, ser vi, at den først tager $2$-tallet og sammenligner med $1$-tallet. Den ser først, at to er større end en, og siden der kun er et element i den korrekte liste, vil den placere to til højre for et. Det næste element, den så tjekker, er tre, som den først sammenligner med et og derefter to for så til sidst at placere det længst til højre. På denne måde indsættes de sidste to elementer i forhold til den korrekte liste.
Vi ser her, at der på samme måde som med bubblesortering køres fire iterationer. Der er i første iteration en sammenligning, derefter to sammenligninger i anden iteration og $n$ sammenligninger i $n$'te iteration. Hermed er denne sortering også $O(n^2)$ i værste fald.


I bedste fald, når listen er $(5,4,3,2,1)$, har den en lineær kompleksitet, da den i alle iterationer kun sammenligner med det element, der er længst til venstre, hvorefter det er indsat korrekt, dermed er kompleksiteten $O(n)$.



%P \\
%NP \\
%NP-Complete \\
%NP-Hard. 


